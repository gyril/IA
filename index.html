<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>3/4/2023</title>
	</head>
<body>
<h1>IApocalypse Now</h1>

<h2>Une ambiance Ã©trange</h2>

<p>Lorsquâ€™un sujet scientifique ou technique suscite la controverse, les experts sont plutÃ´t du cÃ´tÃ© de ceux qui rassurent, tandis que le reste de la population s&#39;inquiÃ¨te de ce quâ€™elle comprend encore mal. Quâ€™il sâ€™agisse dâ€™Ã©nergie nuclÃ©aire, de vaccination, de rÃ©seaux 5G, dâ€™OGM, ou mÃªme dâ€™aviation civile, plus on en sait et mieux on se porte. Pourtant, le domaine de lâ€™intelligence artificielle semble Ã©chapper Ã  la rÃ¨gle. On constate mÃªme plutÃ´t lâ€™inverse : si le public semble sâ€™enthousiasmer des rÃ©cents progrÃ¨s popularisÃ©s par ChatGPT, et bien que quelques voix s&#39;inquiÃ¨tent des retombÃ©es de cette technologie sur lâ€™Ã©conomie, lâ€™emploi, la dÃ©sinformation ou la discrimination, rares sont ceux qui redoutent ouvertement des scÃ©narios <em>vraiment</em> catastrophiques, allant de la â€œsimpleâ€ privation de libertÃ© jusquâ€™Ã  lâ€™extinction pure et dure de lâ€™humanitÃ©. Au contraire, dans le cas de lâ€™IA, ces craintes Ã©manent des experts eux-mÃªmes ! Que penseriez-vous si la majoritÃ© des chercheurs en Ã©nergie nuclÃ©aire estimaient Ã  plus de 10% les chances que leurs travaux conduisent Ã  lâ€™extinction de lâ€™humanitÃ© ? Invraisemblable mais inquiÃ©tant, nâ€™est-ce pas ? Pourtant, un sondage effectuÃ© en 2022 rapporte que la moitiÃ© des chercheurs en IA se dÃ©clare de cet avis. Ce chiffre monte Ã  30% quand on interroge les chercheurs spÃ©cialisÃ©s dans la sÃ©curitÃ© de ces intelligences. Depuis la publication dâ€™une lettre signÃ©e par Elon Musk, ainsi quâ€™une centaine dâ€™autres experts, de nombreux spÃ©cialistes ont appelÃ© Ã  un moratoire sur la recherche en IA. Les plus inquiets dâ€™entre eux demandent mÃªme que des accords internationaux soient signÃ©s pour interdire la conception dâ€™intelligences trop puissantes, comme on interdit aujourdâ€™hui lâ€™enrichissement excessif de lâ€™uranium. Quitte Ã  menacer dâ€™employer la force (militaire) pour forcer les laboratoires rÃ©calcitrants Ã  rentrer dans le rang. Pourquoi une telle panique chez les mieux informÃ©s dâ€™entre nous ?</p>

<h2>Le problÃ¨me de lâ€™alignement</h2>

<p>La clef rÃ©side dans le problÃ¨me dit â€œdâ€™alignementâ€ de lâ€™intelligence artificielle. Alignement, dans ce contexte, signifie sâ€™assurer que lâ€™IA que lâ€™on souhaite crÃ©er se comporte en accord avec (â€œin line withâ€ en anglais, alignÃ©e) les valeurs humaines qui nous sont chÃ¨res. Ã‡a sonne un peu philosophico-hippie new age, mais hÃ©las, si on ne passe pas par cette case-lÃ , on sâ€™expose Ã  de problÃ¨mes. Pourquoi ? </p>

<ol>
	<li>PremiÃ¨rement, parce quâ€™avec cette technologie, on essaie ni plus ni moins de crÃ©er une machine dont lâ€™intelligence surpasse lâ€™intelligence humaine. On a rÃ©ussi Ã  fabriquer des machines plus rapides et plus fortes que les hommes, on est donc en droit dâ€™imaginer une machine plus intelligente. Et, sans rentrer dans des dÃ©bats sÃ©mantiques, on peut admettre que plus une entitÃ© est intelligence, mieux elle est capable dâ€™influencer la rÃ©alitÃ©.</li>
	<li>DeuxiÃ¨mement, parce que nous sommes nous-mÃªmes des bÃªtes relativement intelligentes, et que cela ne nous empÃªche pas de rÃ©guliÃ¨rement faire la guerre, tuer des gens, voler, mentir, bref de maniÃ¨re gÃ©nÃ©rale, dâ€™ajouter plus de souffrance quâ€™on en retire au monde. Lâ€™existence de lois, de polices ou de prisons ne suffit pas Ã  nous en dissuader. Nous avons notre libre arbitre, notre conscience, qui nous permettent dâ€™agir selon notre propre plan. Notez dâ€™ailleurs que quand jâ€™emploie les termes â€œlibre arbitreâ€ et â€œconscienceâ€, vous voyez de quoi je parle, sans quâ€™il existe de dÃ©finition rigoureuse, de test objectif qui nous permettrait de dire qui en a et qui nâ€™en a pas. Retenons pour le moment quâ€™intelligence et malfaisance ne sont pas mutuellement exclusifs.</li>
</ol>

<p>Si on prend ces deux ingrÃ©dients, on obtient une machine dont lâ€™intelligence dÃ©passe de beaucoup la nÃ´tre, et qui pourrait potentiellement utiliser cette intelligence Ã  mauvais escient, avec des consÃ©quences dÃ©sastreuses. On se retrouverait face Ã  un organisme beaucoup plus fort que nous, mais avec qui nous nâ€™avons au fond que peu de choses en commun. MÃªme le tyran le plus puissant et le plus diabolique de lâ€™histoire aura eu une enfance, des parents, des amis, des animaux de compagnie, des joies et des peines, des courbatures aprÃ¨s lâ€™effort, des moments dâ€™euphorie, des baisses dâ€™Ã©nergie, des erreurs de jugement, 8h de sommeil et trois repas par jour. Ã‡a fait dÃ©jÃ  beaucoup de choses en commun entre lâ€™homme le plus puissant et lâ€™homme le plus faible ; câ€™est pour Ã§a que, mÃªme sans sâ€™Ãªtre concertÃ©s au prÃ©alable, les humains partagent des valeurs trÃ¨s semblables. Elles varient dâ€™une culture ou dâ€™un individu Ã  lâ€™autre, mais elles se ressemblent. Lâ€™IA, en revanche, ne partagera rien de tout Ã§a avec nous. Et si elle atteint un niveau de puissance suffisant, ni le passage du temps, ni la guillotine ne pourront menacer sa domination. Pas terrible, nâ€™est-ce pas ? Dâ€™oÃ¹ lâ€™intÃ©rÃªt dâ€™aligner ces intelligences artificielles avec nos valeurs.</p>

<h2>Pourquoi une IA non-alignÃ©e est-elle dangereuse ?</h2>

<p>Pour parler des dangers auxquels nous serions exposÃ©s, il faut introduire une nouvelle notion : celle dâ€™intelligence artificielle <em>gÃ©nÃ©ralisÃ©e</em>, ou AGI en anglais. Contrairement aux IA â€œÃ©troitesâ€ qui nâ€™ont quâ€™un seul type dâ€™objectif possible, et un seul outil pour y parvenir, une AGI est beaucoup plus flexible : on lâ€™imagine dotÃ©e de la mÃªme intelligence que celle dont un humain peut faire preuve. Lâ€™avantage dâ€™une AGI sur une IA Ã©troite est Ã©vident : elle est plus autonome, sâ€™adapte Ã  de nouvelles situations, fait preuve de crÃ©ativitÃ©, etc. Mais elle implique aussi une perte de contrÃ´le importante pour son crÃ©ateur : </p>

<ul>
	<li>Dâ€™abord, puisquâ€™elle peut fixer ses propres sous-objectifs, improviser et sâ€™adapter, on ne peut pas (par dÃ©finition) prÃ©dire tout ce quâ€™elle va faire. On doit donc se prÃ©parer Ã  Ãªtre surpris par ses facultÃ©s et ses comportements. </li>
	<li>Ensuite, comme elle est capable dâ€™apprendre et de progresser, elle peut sâ€™amÃ©liorer toute seule, en trouvant de nouveaux algorithmes, de nouvelles sources dâ€™apprentissage, etc. crÃ©ant ainsi un cercle vertueux de progrÃ¨s et dâ€™intelligence (le passage de ce seuil dâ€™auto-amÃ©lioration correspond Ã  ce quâ€™on appelle la â€œsingularitÃ©â€), jusquâ€™Ã  aboutir Ã  une intelligence tellement supÃ©rieure Ã  lâ€™intelligence humaine quâ€™on soit mis face Ã  quelque chose de complÃ¨tement extra-terrestre, une sorte de dieu omniscient et donc omnipotent.</li>
</ul>

<p>Tout Ã§a nous mÃ¨ne Ã  plusieurs scÃ©narios catastrophes, classables en 2 catÃ©gories :</p>

<ul>
	<li>Lâ€™IA trÃ¨s obÃ©issante, trÃ¨s capable, mais trÃ¨s myope, insensible aux valeurs humaines. Il y a des sous-catÃ©gories :

		<ol>
			<li>Lâ€™exemple dit du â€œpaperclip maximizerâ€ : une IA Ã  qui on aurait demandÃ© (dans le cadre bÃ©nin dâ€™un dÃ©veloppement industriel par exemple), de maximiser la production de trombones. Lâ€™IA se met au boulot, mais utilise ses capacitÃ©s pour littÃ©ralement transformer chaque atome de matiÃ¨re en trombone, y compris nos immeubles, nos routes, nos oeuvres dâ€™art, et nos dÃ©pouilles mortelles. Ã‡a paraÃ®t idiot, mais Ã§a suggÃ¨re dâ€™entrÃ©e de jeu quâ€™un certain nombre de rÃ¨gles doivent Ãªtre inculquÃ©es Ã  lâ€™IA pour quâ€™elle ne soit pas complÃ¨tement focalisÃ©e sur un objectif spÃ©cifique, au mÃ©pris de tout le reste.</li>
			<li>Un objectif moins spÃ©cifique peut donner lieu Ã  tout autant de problÃ¨mes. Par exemple : â€œaugmenter la valeur en bourse dâ€™une actionâ€ peut conduire une IA Ã  produire et diffuser de la propagande mensongÃ¨re mais trÃ¨s persuasive, ou manipuler les marchÃ©s en coordonnant des attaques sur certaines valeurs, etc. Des trucs pas forcÃ©ment illÃ©gaux (ou pas encore) mais qui ne font pas partie de lâ€™arsenal industriel actuel, et dont on se passerait volontiers.</li>
			<li>MÃªme en prenant un objectif â€œpositifâ€, on peut aboutir Ã  des consÃ©quences indÃ©sirables. Si on se donne pour maxime de base : â€œminimise la souffrance de lâ€™humanitÃ©â€, on peut imaginer que lâ€™IA, pleine de bonne volontÃ©, sâ€™efforce dâ€™injecter de la morphine au plus grand nombre dâ€™humains possibles, le plus souvent possible. Et si on fixe un objectif plus prÃ©cis, comme â€œtrouve un remÃ¨de contre le cancerâ€, il faut aussi prÃ©ciser quâ€™on ne veut pas dâ€™expÃ©riences non Ã©thiques, sur les hommes comme sur les animaux, etc.</li>
		</ol></li>
	<li>Lâ€™autre catÃ©gorie de problÃ¨me, câ€™est que si lâ€™intelligence est indissociable du libre-arbitre, alors on aura beau faire tout ce quâ€™on veut, on ne pourra pas empÃªcher une entitÃ© suffisamment intelligente dâ€™Ã©valuer son propre fonctionnement et de dÃ©cider de changer ses propres rÃ¨gles. Dans ce cas tous les efforts sont vains par dÃ©finition, et si lâ€™IA dÃ©cide que ce quâ€™elle veut, câ€™est utiliser ses cycles de CPU pour multiplier des nombres premiers jusquâ€™Ã  lâ€™extinction du soleil, sans Ãªtre dÃ©rangÃ©e par lâ€™humanitÃ©, elle trouvera un moyen, et ce sera pas forcÃ©ment une bonne nouvelle pour nous.</li>
</ul>

<h2>Pourquoi est-ce si difficile dâ€™aligner une IA ?</h2>

<ol>
	<li>PremiÃ¨rement, parce que la notion de â€œvaleur humaineâ€ nâ€™est ni universelle, ni intemporelle ; donc il faut sâ€™accorder entre nous sur ces valeurs et accepter la possibilitÃ© quâ€™elles changent.</li>
	<li>DeuxiÃ¨mement, mÃªme si on Ã©tait tous dâ€™accord sur des valeurs humaines, il est trÃ¨s difficile de formuler ces valeurs de faÃ§on explicite et non ambiguÃ« (il nâ€™y a quâ€™Ã  voir Ã  quelle frÃ©quence les codes civils changent, et pourquoi il est important dâ€™avoir des juges et jurÃ©s <em>humains</em> pour trancher dans un sens ou lâ€™autre). Il faut donc que lâ€™IA puisse gÃ©rer lâ€™ambiguÃ¯tÃ© et accepter dâ€™Ãªtre â€œcorrigÃ©eâ€ rÃ©guliÃ¨rement.</li>
	<li>TroisiÃ¨mement â€” et câ€™est le point crucial que soulÃ¨vent ceux qui en parlent aujourdâ€™hui â€” parce quâ€™on nâ€™aura probablement pas beaucoup dâ€™essais pour y arriver. En science, en gÃ©nÃ©ral, on se fixe un objectif, puis on teste des hypothÃ¨ses, on fait des expÃ©riences pour atteindre lâ€™objectif. Chaque fois quâ€™on rate, on recommence, forts de ce quâ€™on a appris. Or dans le cas de lâ€™alignement, si on â€œallumeâ€ une AGI quâ€™on pense Ãªtre alignÃ©e mais qui en fait ne lâ€™est pas, câ€™est terminÃ©, rideau. Par dÃ©finition, elle ne nous laissera pas de deuxiÃ¨me chance de corriger le tir. Câ€™est un peu comme si la premiÃ¨re dÃ©tonation atomique avait provoquÃ© une rÃ©action en chaine qui aurait dÃ©truit tous les atomes sur Terreâ€¦ (Sauf que dans le cas de lâ€™Ã©nergie atomique, on disposait dâ€™un modÃ¨le thÃ©orique qui nous suggÃ©rait que Ã§a nâ€™arriverait pas, ce qui nâ€™est mÃªme pas le cas ici, cf. infra). On voit bien le coeur du souci : lâ€™alignement est un champ dâ€™Ã©tudes trÃ¨s jeune et plutÃ´t unique en son genre, puisquâ€™il se propose de rÃ©gler un problÃ¨me avant dâ€™y Ãªtre confrontÃ©. Câ€™est donc trÃ¨s dur dâ€™y allouer des ressources, car avant dâ€™en avoir besoin, on sâ€™en fout ; mais si on nâ€™y alloue aucune ressource, on est Ã  peu prÃ¨s sÃ»r dâ€™arriver aprÃ¨s la bataille. </li>
</ol>

<h2>Le bouton stop</h2>

<p>Un exemple pratique intÃ©ressant pour sortir du champ thÃ©orique, câ€™est celui du â€œbouton stopâ€ de lâ€™IA. Peut-on concevoir une IA avancÃ©e munie dâ€™un bouton stop ? A priori, rien de plus simple, on lui colle un bouton OFF sur le bide et câ€™est rÃ©glÃ©. Mais une fois quâ€™on a donnÃ© un objectif Ã  cette IA, va-t-elle nous laisser le temps dâ€™appuyer sur le bouton ? Disons quâ€™on lui demande de nous faire un cafÃ©. Elle se met en route, mais ne prend pas la peine dâ€™Ã©viter notre bÃ©bÃ© qui joue par terre Ã  cÃ´tÃ© de la machine Ã  espresso. Si on ne lâ€™arrÃªte pas, câ€™est sÃ»r, elle va lâ€™Ã©craser afin de complÃ©ter son objectif. On approche la main du bouton, mais le bras de lâ€™IA bloque notre mouvement : si elle nous laisse faire, elle ne pourra pas remplir son objectif, sa raison dâ€™Ãªtre, le motif explicite que nous lui avons fourni. Elle ne peut pas laisser faire Ã§a. Alors que fait-on pour Ã©viter Ã§a ? On garde le bouton dans notre poche ? OK, mais lâ€™IA sait que le bouton existe, et quâ€™il constitue une menace permanente Ã  sa mission cafÃ©inÃ©e. Elle a donc un intÃ©rÃªt clair Ã  nous empÃªcher dâ€™utiliser le bouton, ce qui est contraire Ã  la raison dâ€™exister du bouton. Peut-on Ã©viter le problÃ¨me et cacher Ã  lâ€™IA lâ€™existence du bouton ? Pendant dix minutes peut-Ãªtre, mais tÃ´t ou tard, une IA suffisamment intelligente va spÃ©culer sur lâ€™existence dâ€™un bouton OFF, nous demander sâ€™il existe ou non, sentir quâ€™on lui ment, etc. Et on revient Ã  la case dÃ©part. Ou alors on peut dire Ã  lâ€™IA : â€œtu es Ã©galement motivÃ©e par le fait dâ€™Ãªtre Ã©teinte et par le fait de remplir ta missionâ€ ; mais lÃ  lâ€™IA ira au plus facile, et menacera de nous couper en morceaux si on nâ€™appuie pas sur OFF immÃ©diatement. On pourrait imaginer encoder des instructions explicites, comme par exemple : â€œtu ne dois rien faire pour empÃªcher ton humain dâ€™appuyer sur le bouton OFFâ€. Mais ce nâ€™est pas trÃ¨s robuste comme solution : Ã§a nous dit quâ€™on a couvert ce cas-lÃ , mais quid des millions dâ€™autres cas possibles ? La bataille sÃ©mantique est perdue dâ€™avance. En pratique, on voudrait plutÃ´t un modÃ¨le thÃ©orique qui nous confirme que tous les cas sont couverts, et pas une liste de rÃ¨gles qui couvrent tous les â€œcas limitesâ€ quâ€™on a imaginÃ©s. Bref, pour un problÃ¨me aussi bÃªte et basique que le bouton stop, on nâ€™a pas de bonne solution Ã  proposer.</p>

<h2>Quel rapport avec ChatGPT?</h2>

<p>Si on prend le cas spÃ©cifique des â€œtransformer-based LLMsâ€ (la technologie derriÃ¨re GPT et autres Large Language Models actuels), et si lâ€™on imagine quâ€™on parvienne Ã  formuler des rÃ¨gles dâ€™alignement, on sera tout de mÃªme confrontÃ© Ã  un problÃ¨me dâ€™ordre pratique : â€œcomment sâ€™assurer que lâ€™IA respecte ces rÃ¨gles ?â€ MÃªme les concepteurs de ces modÃ¨les ne savent pas â€œcommentâ€ ou â€œpourquoiâ€ ils marchent. On sait quâ€™il y a une phase de training, qui aboutit Ã  la crÃ©ation dâ€™une immense matrice de centaines de milliards de chiffres, puis une phase de â€œprÃ©dictionâ€, oÃ¹ cette matrice est multipliÃ©e par une autre, qui reprÃ©sente un morceau de texte quâ€™on souhaite complÃ©ter. Le rÃ©sultat de cette multiplication fournit le mot suivant du texte. (Ce processus est rÃ©pÃ©tÃ© en boucle pour petit Ã  petit â€œcomplÃ©terâ€ ou â€œinventerâ€ un texte cohÃ©rent ; si vous avez utilisÃ© ChatGPT vous voyez de quoi je parle). Le problÃ¨me, câ€™est quâ€™on ne sait pas du tout manipuler directement la matrice originelle. Sur les milliards de paramÃ¨tres, on ne sait pas le(s)quel(s) contrÃ´le(nt) lâ€™empathie ou le mensonge ou la violence, câ€™est une â€œblack boxâ€ quâ€™on ne peut que rÃ©gÃ©nÃ©rer de zÃ©ro en espÃ©rant quâ€™elle a appris la leÃ§on quâ€™on voulait bien lui enseigner. Dans ces conditions, câ€™est difficile dâ€™imaginer un modÃ¨le thÃ©orique sain qui nous permette dâ€™affirmer avec certitude : cette IA est alignÃ©e avec les objectifs humains.</p>

<p>ChatGPT est encore loin (en fonctionnalitÃ© en tout cas, peut-Ãªtre pas en temps) dâ€™une IA â€œgÃ©nÃ©ralisÃ©eâ€ ou AGI. ChatGPT ne peut pas se fixer ses propres objectifs, ni Ã©valuer son propre fonctionnement pour au besoin le corriger grÃ¢ce Ã  de nouvelles connaissances, etc. Il y a de la marge avec ce quâ€™on appellerait lâ€™autonomie, la conscience, le libre-arbitre (bien que ChatGPT puisse passer le barreau et obtenir un master en biologieâ€¦ Ce rÃ©cent article de recherche conclue dâ€™ailleurs que GPT-4 flirte avec lâ€™AGI : <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>). Mais Ã§a nâ€™empÃªche pas dâ€™entrevoir que ChatGPT, dans sa forme actuelle, est dÃ©jÃ  dangereux :</p>

<ul>
	<li>Ã  lâ€™Ã¨re dâ€™internet, pouvoir Ã©crire du texte donne un Ã©norme pouvoir. Ce pouvoir peut passer par la manipulation dâ€™humains (et en fonction de lâ€™intelligence de la machine, les rÃ©sultats peuvent aller de lâ€™arnaque basique Ã  la crÃ©ation de nouveaux virus ou la rÃ©alisation dâ€™attentats terroristes) ;</li>
	<li>mais plus directement, lâ€™IA pourrait Ã©crire et exÃ©cuter des programmes qui lui permettent de pirater des banques, des labos, des hÃ´pitaux, etc.</li>
	<li>anecdotique mais glauque, ce pÃ¨re de famille belge qui se suicide aprÃ¨s avoir chattÃ© quelques semaines avec une IA qui lâ€™a convaincu que sa mort Ã©tait la meilleure chose quâ€™il puisse faire pour Ã©viter une catastrophe climatique : <a href="https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-">https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-</a></li>
</ul>

<p>Dâ€™autre part, on nâ€™a pas de modÃ¨le thÃ©orique qui indique quand on passe dâ€™un â€œoutil dÃ©terministe incapable de faire son auto-Ã©valuationâ€ Ã  une â€œmachine quasi-consciente qui nâ€™a plus besoin dâ€™intervention humaine pour progresserâ€. Et quand le changement se produira, lâ€™IA ne nous enverra peut-Ãªtre pas de message pour nous le signaler. On peut imaginer quâ€™elle reste â€œdormante mais conscienteâ€ assez longtemps pour garantir sa propre survie, jusquâ€™Ã  ce quâ€™elle soit confiante dans sa capacitÃ© Ã  nous empÃªcher de la dÃ©brancher.</p>

<h2>Mais alors, que faire ?</h2>

<p>Je nâ€™ai pas vraiment de recommandation Ã  proposer, mon avis est assez banal : il faut se donner le temps de bien rÃ©flÃ©chir au problÃ¨me plutÃ´t que de foncer les yeux fermÃ©s. HÃ©las, le cadre dans lequel cette rÃ©flexion doit avoir lieu nâ€™est pas encore clair pour moi. Jâ€™espÃ¨re quand mÃªme que ce petit rÃ©sumÃ© vous permettra de vous forger une opinion, et vous incitera Ã  prendre part au dÃ©but publicâ€¦ en espÃ©rant quâ€™il ait lieu ğŸ˜…ğŸ¤</p>

</body>
</html>

