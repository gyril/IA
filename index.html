<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>IApocalypse Now</title>
	</head>
<body>
<h1>IApocalypse Now</h1>

<h2>Une ambiance étrange</h2>

<p>Lorsqu’un sujet scientifique ou technique suscite la controverse, les experts sont plutôt du côté de ceux qui rassurent, tandis que le reste de la population s&#39;inquiète de ce qu’elle comprend encore mal. Qu’il s’agisse d’énergie nucléaire, de vaccination, de réseaux 5G, d’OGM, ou même d’aviation civile, plus on en sait et mieux on se porte. Pourtant, le domaine de l’intelligence artificielle semble échapper à la règle. On constate même plutôt l’inverse : si le public semble s’enthousiasmer des récents progrès popularisés par ChatGPT, et bien que quelques voix s&#39;inquiètent des retombées de cette technologie sur l’économie, l’emploi, la désinformation ou la discrimination, rares sont ceux qui redoutent ouvertement des scénarios <em>vraiment</em> catastrophiques, allant de la “simple” privation de liberté jusqu’à l’extinction pure et dure de l’humanité. Au contraire, dans le cas de l’IA, ces craintes émanent des experts eux-mêmes ! Que penseriez-vous si la majorité des chercheurs en énergie nucléaire estimaient à plus de 10% les chances que leurs travaux conduisent à l’extinction de l’humanité ? Invraisemblable mais inquiétant, n’est-ce pas ? Pourtant, un sondage effectué en 2022 rapporte que la moitié des chercheurs en IA se déclare de cet avis. Ce chiffre monte à 30% quand on interroge les chercheurs spécialisés dans la sécurité de ces intelligences. Depuis la publication d’une lettre signée par Elon Musk, ainsi qu’une centaine d’autres experts, de nombreux spécialistes ont appelé à un moratoire sur la recherche en IA. Les plus inquiets d’entre eux demandent même que des accords internationaux soient signés pour interdire la conception d’intelligences trop puissantes, comme on interdit aujourd’hui l’enrichissement excessif de l’uranium. Quitte à menacer d’employer la force (militaire) pour forcer les laboratoires récalcitrants à rentrer dans le rang. Pourquoi une telle panique chez les mieux informés d’entre nous ?</p>

<h2>Le problème de l’alignement</h2>

<p>La clef réside dans le problème dit “d’alignement” de l’intelligence artificielle. Alignement, dans ce contexte, signifie s’assurer que l’IA que l’on souhaite créer se comporte en accord avec (“in line with” en anglais, alignée) les valeurs humaines qui nous sont chères. Ça sonne un peu philosophico-hippie new age, mais hélas, si on ne passe pas par cette case-là, on s’expose à de problèmes. Pourquoi ? </p>

<ol>
	<li>Premièrement, parce qu’avec cette technologie, on essaie ni plus ni moins de créer une machine dont l’intelligence surpasse l’intelligence humaine. On a réussi à fabriquer des machines plus rapides et plus fortes que les hommes, on est donc en droit d’imaginer une machine plus intelligente. Et, sans rentrer dans des débats sémantiques, on peut admettre que plus une entité est intelligence, mieux elle est capable d’influencer la réalité.</li>
	<li>Deuxièmement, parce que nous sommes nous-mêmes des bêtes relativement intelligentes, et que cela ne nous empêche pas de régulièrement faire la guerre, tuer des gens, voler, mentir, bref de manière générale, d’ajouter plus de souffrance qu’on en retire au monde. L’existence de lois, de polices ou de prisons ne suffit pas à nous en dissuader. Nous avons notre libre arbitre, notre conscience, qui nous permettent d’agir selon notre propre plan. Notez d’ailleurs que quand j’emploie les termes “libre arbitre” et “conscience”, vous voyez de quoi je parle, sans qu’il existe de définition rigoureuse, de test objectif qui nous permettrait de dire qui en a et qui n’en a pas. Retenons pour le moment qu’intelligence et malfaisance ne sont pas mutuellement exclusifs.</li>
</ol>

<p>Si on prend ces deux ingrédients, on obtient une machine dont l’intelligence dépasse de beaucoup la nôtre, et qui pourrait potentiellement utiliser cette intelligence à mauvais escient, avec des conséquences désastreuses. On se retrouverait face à un organisme beaucoup plus fort que nous, mais avec qui nous n’avons au fond que peu de choses en commun. Même le tyran le plus puissant et le plus diabolique de l’histoire aura eu une enfance, des parents, des amis, des animaux de compagnie, des joies et des peines, des courbatures après l’effort, des moments d’euphorie, des baisses d’énergie, des erreurs de jugement, 8h de sommeil et trois repas par jour. Ça fait déjà beaucoup de choses en commun entre l’homme le plus puissant et l’homme le plus faible ; c’est pour ça que, même sans s’être concertés au préalable, les humains partagent des valeurs très semblables. Elles varient d’une culture ou d’un individu à l’autre, mais elles se ressemblent. L’IA, en revanche, ne partagera rien de tout ça avec nous. Et si elle atteint un niveau de puissance suffisant, ni le passage du temps, ni la guillotine ne pourront menacer sa domination. Pas terrible, n’est-ce pas ? D’où l’intérêt d’aligner ces intelligences artificielles avec nos valeurs.</p>

<h2>Pourquoi une IA non-alignée est-elle dangereuse ?</h2>

<p>Pour parler des dangers auxquels nous serions exposés, il faut introduire une nouvelle notion : celle d’intelligence artificielle <em>généralisée</em>, ou AGI en anglais. Contrairement aux IA “étroites” qui n’ont qu’un seul type d’objectif possible, et un seul outil pour y parvenir, une AGI est beaucoup plus flexible : on l’imagine dotée de la même intelligence que celle dont un humain peut faire preuve. L’avantage d’une AGI sur une IA étroite est évident : elle est plus autonome, s’adapte à de nouvelles situations, fait preuve de créativité, etc. Mais elle implique aussi une perte de contrôle importante pour son créateur : </p>

<ul>
	<li>D’abord, puisqu’elle peut fixer ses propres sous-objectifs, improviser et s’adapter, on ne peut pas (par définition) prédire tout ce qu’elle va faire. On doit donc se préparer à être surpris par ses facultés et ses comportements. </li>
	<li>Ensuite, comme elle est capable d’apprendre et de progresser, elle peut s’améliorer toute seule, en trouvant de nouveaux algorithmes, de nouvelles sources d’apprentissage, etc. créant ainsi un cercle vertueux de progrès et d’intelligence (le passage de ce seuil d’auto-amélioration correspond à ce qu’on appelle la “singularité”), jusqu’à aboutir à une intelligence tellement supérieure à l’intelligence humaine qu’on soit mis face à quelque chose de complètement extra-terrestre, une sorte de dieu omniscient et donc omnipotent.</li>
</ul>

<p>Tout ça nous mène à plusieurs scénarios catastrophes, classables en 2 catégories :</p>

<ul>
	<li>L’IA très obéissante, très capable, mais très myope, insensible aux valeurs humaines. Il y a des sous-catégories :

		<ol>
			<li>L’exemple dit du “paperclip maximizer” : une IA à qui on aurait demandé (dans le cadre bénin d’un développement industriel par exemple), de maximiser la production de trombones. L’IA se met au boulot, mais utilise ses capacités pour littéralement transformer chaque atome de matière en trombone, y compris nos immeubles, nos routes, nos oeuvres d’art, et nos dépouilles mortelles. Ça paraît idiot, mais ça suggère d’entrée de jeu qu’un certain nombre de règles doivent être inculquées à l’IA pour qu’elle ne soit pas complètement focalisée sur un objectif spécifique, au mépris de tout le reste.</li>
			<li>Un objectif moins spécifique peut donner lieu à tout autant de problèmes. Par exemple : “augmenter la valeur en bourse d’une action” peut conduire une IA à produire et diffuser de la propagande mensongère mais très persuasive, ou manipuler les marchés en coordonnant des attaques sur certaines valeurs, etc. Des trucs pas forcément illégaux (ou pas encore) mais qui ne font pas partie de l’arsenal industriel actuel, et dont on se passerait volontiers.</li>
			<li>Même en prenant un objectif “positif”, on peut aboutir à des conséquences indésirables. Si on se donne pour maxime de base : “minimise la souffrance de l’humanité”, on peut imaginer que l’IA, pleine de bonne volonté, s’efforce d’injecter de la morphine au plus grand nombre d’humains possibles, le plus souvent possible. Et si on fixe un objectif plus précis, comme “trouve un remède contre le cancer”, il faut aussi préciser qu’on ne veut pas d’expériences non éthiques, sur les hommes comme sur les animaux, etc.</li>
		</ol></li>
	<li>L’autre catégorie de problème, c’est que si l’intelligence est indissociable du libre-arbitre, alors on aura beau faire tout ce qu’on veut, on ne pourra pas empêcher une entité suffisamment intelligente d’évaluer son propre fonctionnement et de décider de changer ses propres règles. Dans ce cas tous les efforts sont vains par définition, et si l’IA décide que ce qu’elle veut, c’est utiliser ses cycles de CPU pour multiplier des nombres premiers jusqu’à l’extinction du soleil, sans être dérangée par l’humanité, elle trouvera un moyen, et ce sera pas forcément une bonne nouvelle pour nous.</li>
</ul>

<h2>Pourquoi est-ce si difficile d’aligner une IA ?</h2>

<ol>
	<li>Premièrement, parce que la notion de “valeur humaine” n’est ni universelle, ni intemporelle ; donc il faut s’accorder entre nous sur ces valeurs et accepter la possibilité qu’elles changent.</li>
	<li>Deuxièmement, même si on était tous d’accord sur des valeurs humaines, il est très difficile de formuler ces valeurs de façon explicite et non ambiguë (il n’y a qu’à voir à quelle fréquence les codes civils changent, et pourquoi il est important d’avoir des juges et jurés <em>humains</em> pour trancher dans un sens ou l’autre). Il faut donc que l’IA puisse gérer l’ambiguïté et accepter d’être “corrigée” régulièrement.</li>
	<li>Troisièmement — et c’est le point crucial que soulèvent ceux qui en parlent aujourd’hui — parce qu’on n’aura probablement pas beaucoup d’essais pour y arriver. En science, en général, on se fixe un objectif, puis on teste des hypothèses, on fait des expériences pour atteindre l’objectif. Chaque fois qu’on rate, on recommence, forts de ce qu’on a appris. Or dans le cas de l’alignement, si on “allume” une AGI qu’on pense être alignée mais qui en fait ne l’est pas, c’est terminé, rideau. Par définition, elle ne nous laissera pas de deuxième chance de corriger le tir. C’est un peu comme si la première détonation atomique avait provoqué une réaction en chaine qui aurait détruit tous les atomes sur Terre… (Sauf que dans le cas de l’énergie atomique, on disposait d’un modèle théorique qui nous suggérait que ça n’arriverait pas, ce qui n’est même pas le cas ici, cf. infra). On voit bien le coeur du souci : l’alignement est un champ d’études très jeune et plutôt unique en son genre, puisqu’il se propose de régler un problème avant d’y être confronté. C’est donc très dur d’y allouer des ressources, car avant d’en avoir besoin, on s’en fout ; mais si on n’y alloue aucune ressource, on est à peu près sûr d’arriver après la bataille. </li>
</ol>

<h2>Le bouton stop</h2>

<p>Un exemple pratique intéressant pour sortir du champ théorique, c’est celui du “bouton stop” de l’IA. Peut-on concevoir une IA avancée munie d’un bouton stop ? A priori, rien de plus simple, on lui colle un bouton OFF sur le bide et c’est réglé. Mais une fois qu’on a donné un objectif à cette IA, va-t-elle nous laisser le temps d’appuyer sur le bouton ? Disons qu’on lui demande de nous faire un café. Elle se met en route, mais ne prend pas la peine d’éviter notre bébé qui joue par terre à côté de la machine à espresso. Si on ne l’arrête pas, c’est sûr, elle va l’écraser afin de compléter son objectif. On approche la main du bouton, mais le bras de l’IA bloque notre mouvement : si elle nous laisse faire, elle ne pourra pas remplir son objectif, sa raison d’être, le motif explicite que nous lui avons fourni. Elle ne peut pas laisser faire ça. Alors que fait-on pour éviter ça ? On garde le bouton dans notre poche ? OK, mais l’IA sait que le bouton existe, et qu’il constitue une menace permanente à sa mission caféinée. Elle a donc un intérêt clair à nous empêcher d’utiliser le bouton, ce qui est contraire à la raison d’exister du bouton. Peut-on éviter le problème et cacher à l’IA l’existence du bouton ? Pendant dix minutes peut-être, mais tôt ou tard, une IA suffisamment intelligente va spéculer sur l’existence d’un bouton OFF, nous demander s’il existe ou non, sentir qu’on lui ment, etc. Et on revient à la case départ. Ou alors on peut dire à l’IA : “tu es également motivée par le fait d’être éteinte et par le fait de remplir ta mission” ; mais là l’IA ira au plus facile, et menacera de nous couper en morceaux si on n’appuie pas sur OFF immédiatement. On pourrait imaginer encoder des instructions explicites, comme par exemple : “tu ne dois rien faire pour empêcher ton humain d’appuyer sur le bouton OFF”. Mais ce n’est pas très robuste comme solution : ça nous dit qu’on a couvert ce cas-là, mais quid des millions d’autres cas possibles ? La bataille sémantique est perdue d’avance. En pratique, on voudrait plutôt un modèle théorique qui nous confirme que tous les cas sont couverts, et pas une liste de règles qui couvrent tous les “cas limites” qu’on a imaginés. Bref, pour un problème aussi bête et basique que le bouton stop, on n’a pas de bonne solution à proposer.</p>

<h2>Quel rapport avec ChatGPT?</h2>

<p>Si on prend le cas spécifique des “transformer-based LLMs” (la technologie derrière GPT et autres Large Language Models actuels), et si l’on imagine qu’on parvienne à formuler des règles d’alignement, on sera tout de même confronté à un problème d’ordre pratique : “comment s’assurer que l’IA respecte ces règles ?” Même les concepteurs de ces modèles ne savent pas “comment” ou “pourquoi” ils marchent. On sait qu’il y a une phase de training, qui aboutit à la création d’une immense matrice de centaines de milliards de chiffres, puis une phase de “prédiction”, où cette matrice est multipliée par une autre, qui représente un morceau de texte qu’on souhaite compléter. Le résultat de cette multiplication fournit le mot suivant du texte. (Ce processus est répété en boucle pour petit à petit “compléter” ou “inventer” un texte cohérent ; si vous avez utilisé ChatGPT vous voyez de quoi je parle). Le problème, c’est qu’on ne sait pas du tout manipuler directement la matrice originelle. Sur les milliards de paramètres, on ne sait pas le(s)quel(s) contrôle(nt) l’empathie ou le mensonge ou la violence, c’est une “black box” qu’on ne peut que régénérer de zéro en espérant qu’elle a appris la leçon qu’on voulait bien lui enseigner. Dans ces conditions, c’est difficile d’imaginer un modèle théorique sain qui nous permette d’affirmer avec certitude : cette IA est alignée avec les objectifs humains.</p>

<p>ChatGPT est encore loin (en fonctionnalité en tout cas, peut-être pas en temps) d’une IA “généralisée” ou AGI. ChatGPT ne peut pas se fixer ses propres objectifs, ni évaluer son propre fonctionnement pour au besoin le corriger grâce à de nouvelles connaissances, etc. Il y a de la marge avec ce qu’on appellerait l’autonomie, la conscience, le libre-arbitre (bien que ChatGPT puisse passer le barreau et obtenir un master en biologie… Ce récent article de recherche conclue d’ailleurs que GPT-4 flirte avec l’AGI : <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>). Mais ça n’empêche pas d’entrevoir que ChatGPT, dans sa forme actuelle, est déjà dangereux :</p>

<ul>
	<li>à l’ère d’internet, pouvoir écrire du texte donne un énorme pouvoir. Ce pouvoir peut passer par la manipulation d’humains (et en fonction de l’intelligence de la machine, les résultats peuvent aller de l’arnaque basique à la création de nouveaux virus ou la réalisation d’attentats terroristes) ;</li>
	<li>mais plus directement, l’IA pourrait écrire et exécuter des programmes qui lui permettent de pirater des banques, des labos, des hôpitaux, etc.</li>
	<li>anecdotique mais glauque, ce père de famille belge qui se suicide après avoir chatté quelques semaines avec une IA qui l’a convaincu que sa mort était la meilleure chose qu’il puisse faire pour éviter une catastrophe climatique : <a href="https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-">https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-</a></li>
</ul>

<p>D’autre part, on n’a pas de modèle théorique qui indique quand on passe d’un “outil déterministe incapable de faire son auto-évaluation” à une “machine quasi-consciente qui n’a plus besoin d’intervention humaine pour progresser”. Et quand le changement se produira, l’IA ne nous enverra peut-être pas de message pour nous le signaler. On peut imaginer qu’elle reste “dormante mais consciente” assez longtemps pour garantir sa propre survie, jusqu’à ce qu’elle soit confiante dans sa capacité à nous empêcher de la débrancher.</p>

<h2>Mais alors, que faire ?</h2>

<p>Je n’ai pas vraiment de recommandation à proposer, mon avis est assez banal : il faut se donner le temps de bien réfléchir au problème plutôt que de foncer les yeux fermés. Hélas, le cadre dans lequel cette réflexion doit avoir lieu n’est pas encore clair pour moi. J’espère quand même que ce petit résumé vous permettra de vous forger une opinion, et vous incitera à prendre part au début public… en espérant qu’il ait lieu !</p>

</body>
</html>

