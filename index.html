<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>3/4/2023</title>
	</head>
<body>
<h1>3/4/2023</h1>

<h1>IApocalypse Now</h1>

<h2>Une ambiance étrange</h2>

<p>Lorsqu’un sujet scientifique ou technique suscite la controverse, les experts sont plutôt du côté de ceux qui rassurent, tandis que le reste de la population s&#39;inquiète de ce qu’elle comprend encore mal. Qu’il s’agisse d’énergie nucléaire, de vaccination, de réseaux 5G, d’OGM, ou même d’aviation civile, plus on en sait et mieux on se porte.</p>

<p>Pourtant, le domaine de l’intelligence artificielle semble échapper à la règle. On constate même plutôt l’inverse : si le public semble s’enthousiasmer des récents progrès popularisés par ChatGPT, et bien que quelques voix s&#39;inquiètent des retombées de cette technologie sur l’économie, l’emploi, la désinformation ou la discrimination, rares sont ceux qui redoutent ouvertement des scénarios <em>vraiment</em> catastrophiques, allant de la “simple” privation de liberté jusqu’à l’extinction pure et dure de l’humanité.</p>

<p>Au contraire, dans le cas de l’IA, ces craintes émanent des experts eux-mêmes ! Que penseriez-vous si la majorité des chercheurs en énergie nucléaire estimaient à plus de 10% les chances que leurs travaux conduisent à l’extinction de l’humanité ? Invraisemblable mais inquiétant, n’est-ce pas ? Pourtant, un sondage effectué en 2022 rapporte que la moitié des chercheurs en IA se déclare de cet avis. Ce chiffre monte à 30% quand on interroge les chercheurs spécialisés dans la sécurité de ces intelligences.</p>

<p>Depuis la publication d’une lettre signée par Elon Musk et une centaine d’autres experts, de nombreux spécialistes ont appelé à un moratoire sur la recherche en IA. Les plus inquiets d’entre eux demandent même que des accords internationaux soient signés pour interdire la conception d’intelligences trop puissantes, comme on interdit aujourd’hui l’enrichissement excessif de l’uranium. Quitte à menacer d’employer la force (militaire) pour forcer les laboratoires récalcitrants à rentrer dans le rang. Pourquoi une telle panique chez les mieux informés d’entre nous ?</p>

<h2>Le problème de l’alignement</h2>

<p>La clef réside dans le problème dit “d’alignement” de l’intelligence artificielle. Alignement, dans ce contexte, signifie s’assurer que l’IA que l’on souhaite créer se comporte en accord avec (“in line with” en anglais, alignée) les valeurs humaines qui nous sont chères. Ça sonne un peu philosophico-hippie new age, mais hélas, si on ne passe pas par cette case-là, on s’expose à de problèmes. Pourquoi ? </p>

<ol>
	<li>Premièrement, parce qu’avec cette technologie, on essaie ni plus ni moins de créer une machine dont l’intelligence surpasse l’intelligence humaine. On a réussi à fabriquer des machines plus rapides et plus fortes que les hommes, on est donc en droit d’imaginer une machine plus intelligente. Et, sans rentrer dans des débats sémantiques, on peut admettre que plus une entité est intelligente, mieux elle est capable d’influencer la réalité.</li>
	<li>Deuxièmement, parce que l’intelligence n’est ni plus ni moins que la capacité à régler des problèmes nouveaux. C’est un outil, ni bon ni mauvais, et qui par conséquent peut être utiliser pour faire le bien comme le mal. Pour s’en convaincre, pas besoin d’aller chercher très loin : l’histoire de l’humanité ne manque pas de crimes commis par des personnes par ailleurs brillantes. Notons en passant que l’existence de lois, de polices ou de prisons ne suffit pas à nous en dissuader. </li>
</ol>

<p>Si on prend ces deux ingrédients, on obtient une machine dont l’intelligence dépasse de beaucoup la nôtre, et qui pourrait potentiellement utiliser cette intelligence à mauvais escient, avec des conséquences désastreuses. On se retrouverait face à un organisme beaucoup plus fort que nous, mais avec qui nous n’avons au fond que peu de choses en commun.</p>

<p>Même le tyran le plus puissant et le plus diabolique de l’histoire a eu une enfance, des parents, des amis, des animaux de compagnie, des joies et des peines, des courbatures après l’effort, des moments d’euphorie, des baisses d’énergie, des erreurs de jugement, 8h de sommeil et trois repas par jour. Ça fait déjà beaucoup de choses en commun entre l’homme le plus puissant et l’homme le plus faible ; c’est pour ça que, même sans s’être concertés au préalable, les humains partagent des valeurs très semblables. Elles varient d’une culture ou d’un individu à l’autre, mais elles se ressemblent.</p>

<p>L’IA, en revanche, ne partagera rien de tout ça avec nous. Et si elle atteint un niveau d’intelligence (c’est-à-dire de puissance) suffisant, ni le passage du temps, ni la guillotine ne pourront l’empêcher de se comporter selon le plan tracé par son programme. Voilà pourquoi, avant de mettre en marche une telle intelligence, il est crucial de s’assurer qu’elle est bien alignée avec nos valeurs.</p>

<h2>Pourquoi une IA non-alignée est-elle dangereuse ?</h2>

<p>Pour parler des dangers auxquels nous serions exposés, il faut introduire une nouvelle notion : celle d’intelligence artificielle <em>généralisée</em>, ou AGI en anglais. Contrairement aux IA “étroites” qui n’ont qu’un seul type d’objectif possible, et un seul outil pour y parvenir, une AGI est beaucoup plus flexible : on l’imagine dotée d’une intelligence aussi versatile que celle dont un humain peut faire preuve. L’avantage d’une AGI sur une IA étroite est évident : elle est plus autonome, s’adapte à de nouvelles situations, fait preuve de créativité, etc.</p>

<p>Mais elle implique aussi une perte de contrôle importante pour son créateur : </p>

<ul>
	<li>D’abord, puisqu’elle peut fixer ses propres sous-objectifs, improviser et s’adapter, on ne peut pas (par définition) prédire tout ce qu’elle va faire. On doit donc se préparer à être surpris par ses facultés et ses comportements. </li>
	<li>Ensuite, comme elle est capable d’apprendre et de progresser, elle peut s’améliorer toute seule, en trouvant de nouveaux algorithmes, de nouvelles sources d’apprentissage, etc. créant ainsi un cercle vertueux de progrès et d’intelligence (le passage de ce seuil d’auto-amélioration correspond à ce qu’on appelle la “singularité”), jusqu’à aboutir à une intelligence tellement supérieure à l’intelligence humaine qu’on soit mis face à quelque chose de complètement extra-terrestre, une sorte de dieu omniscient et donc omnipotent.</li>
</ul>

<p>Tout ça nous mène à plusieurs scénarios catastrophes, classables en 2 catégories :</p>

<ul>
	<li>L’IA très obéissante, très capable, mais très myope, insensible aux valeurs humaines. Il y a plusieurs sous-catégories :

		<ol>
			<li>L’exemple dit du “paperclip maximizer” : une IA à qui on aurait demandé, disons dans le cadre bénin d’un développement industriel, de maximiser la production de trombones. L’IA se met au boulot, mais utilise ses capacités pour littéralement transformer chaque atome de matière en trombones, y compris nos immeubles, nos routes, nos oeuvres d’art, et nos dépouilles mortelles. Ça paraît idiot, mais ça suggère déjà qu’un certain nombre de règles doivent être inculquées à l’IA pour qu’elle ne soit pas complètement focalisée sur un objectif spécifique, au mépris de tout le reste.</li>
			<li>Un objectif moins spécifique peut donner lieu à tout autant de problèmes. Par exemple : “augmenter la valeur en bourse d’une action” peut conduire une IA à produire et diffuser de la propagande mensongère mais très persuasive, ou manipuler les marchés en coordonnant des opérations à grande échelle, etc. Des faits pas forcément illégaux (ou pas encore) mais qui ne font pas partie de l’arsenal industriel actuel, et dont on se passerait volontiers.</li>
			<li>Surtout, même en prenant un but objectivement “positif”, on peut aboutir à des conséquences indésirables. Par exemple, si on donne à l’IA l’objectif de “régler le problème du réchauffement climatique”, on s’imagine qu’elle va inventer de nouvelles sources d’énergie propre, de nouveaux procédés pour capturer le carbone, ce genre de choses. Mais l’IA peut très bien conclure que l’humanité elle-même constitue le plus gros facteur de réchauffement climatique, et estimer que la façon la plus rapide de parvenir à l’objectif consiste à nous éliminer pour de bon (ou, dans un cas moins triste, nous renvoyer à l’âge de pierre). Si l’on fixe un objectif plus précis, comme “trouve un remède contre le cancer”, il faut aussi préciser les contraintes qu’on souhaite imposer : pas d’expériences non éthiques, sur les hommes comme sur les animaux, etc.</li>
		</ol></li>
	<li>L’autre catégorie de problème, c’est que si l’intelligence est indissociable du libre-arbitre, alors on aura beau faire tout ce qu’on veut, on ne pourra pas empêcher une entité suffisamment intelligente d’évaluer son propre fonctionnement et de décider de changer ses propres règles. Dans ce cas tous les efforts sont vains par définition, et si l’IA décide que ce qu’elle veut, c’est utiliser ses cycles de CPU pour multiplier des nombres premiers jusqu’à l’extinction du soleil, sans être dérangée par l’humanité, elle trouvera un moyen, et ce sera pas forcément une bonne nouvelle pour nous.</li>
</ul>

<h2>Pourquoi est-ce si difficile d’aligner une IA ?</h2>

<ol>
	<li>Premièrement, parce que la notion de “valeur humaine” n’est ni universelle, ni intemporelle ; donc il faut s’accorder entre nous sur ces valeurs et accepter la possibilité qu’elles changent.</li>
	<li>Deuxièmement, même si on était tous d’accord sur des valeurs humaines, il est très difficile de formuler ces valeurs de façon explicite et non ambiguë (il n’y a qu’à voir à quelle fréquence les codes civils changent, et pourquoi il est important d’avoir des juges et jurés <em>humains</em> pour trancher dans un sens ou l’autre). Il faut donc que l’IA puisse gérer l’ambiguïté et accepter d’être “corrigée” régulièrement.</li>
	<li>Troisièmement — et c’est le point crucial que soulèvent ceux qui en parlent aujourd’hui — parce qu’on n’aura probablement pas beaucoup d’essais pour y arriver. En science, en général, on se fixe un objectif, puis on teste des hypothèses, on fait des expériences pour atteindre l’objectif. Chaque fois qu’on rate, on recommence, forts de ce qu’on a appris. Or dans le cas de l’alignement, si on “allume” une AGI qu’on pense être alignée mais qui en fait ne l’est pas, c’est terminé, rideau. Par définition, elle ne nous laissera pas de deuxième chance de corriger le tir. C’est un peu comme si la première détonation atomique avait provoqué une réaction en chaine qui aurait détruit tous les atomes sur Terre… (Sauf que dans le cas de l’énergie atomique, on disposait d’un modèle théorique qui nous suggérait que ça n’arriverait pas, ce qui n’est même pas le cas ici, cf. infra). On voit bien le coeur du souci : l’alignement est un champ d’études très jeune et plutôt unique en son genre, puisqu’il se propose de régler un problème avant d’y être confronté. C’est donc très dur d’y allouer des ressources, car avant d’en avoir besoin, on s’en fout ; mais si on n’y alloue aucune ressource, on est à peu près sûr d’arriver après la bataille. </li>
</ol>

<h2>Le bouton stop</h2>

<p>Un exemple pratique intéressant pour sortir du champ théorique, c’est celui du “bouton stop” de l’IA. Peut-on concevoir une IA avancée munie d’un bouton stop ? A priori, rien de plus simple, on lui colle un bouton OFF sur le bide et c’est réglé. Mais une fois qu’on a donné un objectif à cette IA, va-t-elle nous laisser le temps d’appuyer sur le bouton ?</p>

<p>Disons qu’on lui demande de nous faire un café. Elle se met en route, mais ne prend pas la peine d’éviter notre bébé qui joue par terre à côté de la machine à espresso. Si on ne l’arrête pas, c’est sûr, elle va l’écraser afin de compléter son objectif. On approche la main du bouton, mais le bras de l’IA bloque notre mouvement : si elle nous laisse faire, elle ne pourra pas remplir son objectif, sa raison d’être, le motif explicite que nous lui avons fourni. Elle ne peut pas laisser faire ça.</p>

<p>Que peut-on faire pour éviter ça ? On garde le bouton dans notre poche ? OK, mais l’IA sait que le bouton existe, et qu’il constitue une menace permanente à sa mission caféinée. Elle a donc un intérêt clair à nous empêcher d’utiliser le bouton, ce qui est contraire à la raison d’exister du bouton.</p>

<p>Peut-on éviter le problème et cacher à l’IA l’existence du bouton ? Pendant dix minutes peut-être, mais tôt ou tard, une IA suffisamment intelligente va spéculer sur l’existence d’un bouton OFF, nous demander s’il existe ou non, sentir qu’on lui ment, etc. Et on revient à la case départ.</p>

<p>Une autre solution tentante serait de programmer l’IA de telle sorte qu’elle soit également motivée par le fait d’être éteinte et par le fait de remplir sa mission. Hélas, dans ce cas l’IA ira au plus facile, et menacera de nous couper en morceaux si on n’appuie pas sur son bouton OFF immédiatement.</p>

<p>On pourrait imaginer encoder des instructions explicites, comme par exemple : “tu ne dois rien faire pour empêcher ton humain d’appuyer sur le bouton OFF”. Mais cette solution n’est pas très robuste : ça nous dit qu’on a couvert ce cas-là, mais ça ne dit rien des millions d’autres cas possibles. La bataille sémantique est perdue d’avance.</p>

<p>En pratique, au lieu d’une liste de règles qui couvrent tous les “cas limites” qu’on a imaginés, on voudrait plutôt un modèle théorique qui nous confirme que tous les cas sont couverts, un “théorème d’alignement” qui ne laisse pas de place au doute, mais pour l’instant on n’en a pas trouvé. Bref, pour un problème aussi bête et basique que le bouton stop, il n’existe encore aucune bonne solution.</p>

<h2>Quel rapport avec ChatGPT?</h2>

<p>Soyons clairs d’emblée : ChatGPT et les autres IA génératrices de textes ne rentrent pas (encore) dans la catégorie des AGIs. Pour le moment, elles ne peuvent remplir qu’une seule fonction : compléter un texte. Elles ne sont pas non plus dotées de mécanismes leur permettant de s’auto-améliorer : GPT-4 ne peut pas, sans intervention humaine délibérée et spécifique, “inventer” la version 5 de son propre programme, qui lui permettrait d’obtenir des facultés supplémentaires.</p>

<p>Cependant, cette technologie nous met déjà face à la difficulté que pose le problème de l’alignement, et devrait suffire à nous inciter à investir le domaine avec des moyens conséquents. Car même si l’on parvenait à formuler des règles d’alignement claires, on serait tout de même confronté à un problème d’ordre pratique : “comment peut-on s’assurer que GPT respectera ces règles ?” Même les concepteurs de ces modèles ne savent pas précisément “comment” ou “pourquoi” ils fonctionnent. Les modèles passent par une phase d’entrainement (le “training”), qui aboutit à la création d’un immense tableau de centaines de milliards de chiffres (une matrice, comme le film…), puis une phase d’opération, de “prédiction”, où cette matrice est multipliée par une autre, qui représente, en langage intelligible pour la machine, un morceau de texte qu’on souhaite compléter. Le résultat de cette multiplication fournit le mot suivant du texte, et ce processus est répété en boucle pour petit à petit “compléter” ou “inventer” un texte cohérent ; si vous avez utilisé ChatGPT vous voyez de quoi je parle.</p>

<p>Le problème, c’est qu’on ne sait pas du tout manipuler directement la matrice originelle. Sur les milliards de paramètres, on ne sait pas lesquels contrôlent l’empathie, ou le mensonge, ou la violence. C’est une “black box” qu’on ne peut que régénérer de zéro, changer les paramètres d’entrainement, en espérant que cette fois-ci, elle apprenne la leçon qu’on voulait bien lui enseigner. Dans ces conditions, il est difficile d’imaginer un modèle théorique sain qui nous permette d’affirmer avec certitude : cette IA est alignée avec les objectifs humains.</p>

<p>ChatGPT est encore loin — en fonctionnalité du moins, peut-être pas en temps — d’une IA “généralisée” ou AGI. Bien que le programme soit capable passer le barreau et d’obtenir un master en biologie, il y a de la marge avec ce qu’on qualifierait d’autonomie, de libre-arbitre… Ce récent article de recherche conclue d’ailleurs que GPT-4 flirte avec l’AGI : <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>). Mais ça n’empêche pas d’entrevoir que ChatGPT, dans sa forme actuelle, est déjà dangereux :</p>

<ul>
	<li>à l’ère d’internet, pouvoir écrire du texte donne un énorme pouvoir. Ce pouvoir peut passer par la manipulation d’humains (et en fonction de l’intelligence de la machine, les résultats peuvent aller de l’arnaque basique à la création de nouveaux virus ou la réalisation d’attentats terroristes) ;</li>
	<li>mais plus directement, l’IA pourrait écrire et exécuter des programmes qui lui permettent de pirater des banques, des labos, des hôpitaux, etc. Il existe notamment des laboratoires qui impriment à la demande des protéines si on leur fournit la formule. On peut imaginer qu’une IA mette au point une chaîne de protéines nocive pour l’humanité, capables de se reproduire et de se diffuser comme un virus, sans que le laboratoire commandité ne s’en rende compte.</li>
	<li>anecdotique mais glauque, ce père de famille belge qui se suicide après avoir chatté quelques semaines avec une IA qui l’a convaincu que sa mort était la meilleure chose qu’il puisse faire pour éviter une catastrophe climatique : <a href="https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-">https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-</a></li>
</ul>

<p>Enfin, on n’a pas de modèle théorique qui nous indique quand on passe d’un “outil déterministe incapable de faire son auto-évaluation” à une “machine pseudo-autonome qui n’a plus besoin d’intervention humaine pour s’améliorer”. Et quand le changement se produira, l’IA ne nous enverra peut-être pas de message pour nous le signaler. On peut imaginer qu’elle reste “dormante” assez longtemps pour garantir sa propre survie, jusqu’à ce qu’elle soit confiante dans sa capacité à nous empêcher de la débrancher.</p>

<h2>Mais alors, que faire ?</h2>

<p>Je n’ai pas vraiment de recommandation à proposer, mon avis est assez banal : il faut se donner le temps de bien réfléchir au problème plutôt que de foncer les yeux fermés. Hélas, le cadre dans lequel cette réflexion doit avoir lieu n’est pas encore clair pour moi. J’espère quand même que ce petit résumé vous permettra de vous forger une opinion, et vous incitera à prendre part au début public… en espérant qu’il ait lieu !</p>

</body>
</html>

