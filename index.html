<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>Le problème de l’alignement</title>
	</head>
<body>
<h1>Le problème de l’alignement</h1>

<p>Le premier point important de la discussion, c’est de s’accorder sur le fait que le problème dit “d’alignement” de l’intelligence artificielle est un problème important. Alignement signifie ici s’assurer que l’IA que l’on souhaite créer se comporte en accord avec (“in line with”, alignée) les valeurs humaines qui nous sont chères. Ça paraît un peu philosophico-hippie new age, mais si on ne passe pas par cette case-là, on s’expose à pas mal de problèmes. Pourquoi ? </p>

<ol>
	<li>Premièrement, parce qu’au fond, avec cette technologie, on essaie de créer une machine dont l’intelligence surpasse l’intelligence humaine. On a déjà réussi à faire des machines plus rapides et plus fortes que les hommes, alors on est en droit d’imaginer une machine plus intelligente. Et sans rentrer dans des débats sémantiques, on peut admettre que plus une entité est intelligence, mieux elle est capable d’influencer la réalité.</li>
	<li>Deuxièmement, parce que nous sommes nous-mêmes des bêtes relativement intelligentes, et que ça ne nous empêche pas de régulièrement faire la guerre, tuer des gens, voler, mentir, bref de manière générale, d’ajouter plus de souffrance qu’on en retire au monde. L’existence de lois, de polices, de prisons, ne suffit pas à nous en dissuader. On a notre libre arbitre, notre conscience, qui nous permettent d’agir selon notre propre plan. Notez d’ailleurs que quand j’emploie les termes “libre arbitre” et “conscience”, vous voyez de quoi je parle, mais au fond il n’y a pas de définition rigoureuse, pas de test objectif qui nous permettrait de dire qui en a et qui n’en a pas. Bref, intelligence et malfaisance ne sont pas mutuellement exclusifs.</li>
</ol>

<p>Si on prend ces deux ingrédients, on obtient une machine dont l’intelligence dépasse largement la nôtre, et qui pourrait potentiellement utiliser cette intelligence à mauvais escient, avec des conséquences désastreuses. Pas cool, n’est-ce pas ? D’où l’intérêt d’aligner ces intelligences artificielles avec les valeurs humaines.</p>

<h2>Pourquoi une IA non-alignée est dangereuse ?</h2>

<p>Pour parler des dangers associés, il faut ici introduire une nouvelle notion, celle d’intelligence artificielle <em>généralisée</em>, ou AGI en anglais. Contrairement aux IA “étroites” qui n’ont qu’un type d’objectif possible et un seul outil pour y parvenir, une AGI est beaucoup plus flexible, grosso modo dotée de la même intelligence que celle dont un humain peut faire preuve. L’avantage d’une AGI est évident : elle est très autonome, s’adapte à de nouvelles situations, fait preuve de créativité. Mais elle implique aussi une perte de contrôle importante pour son créateur : </p>

<ul>
	<li>D’abord, comme elle peut fixer ses propres sous-objectifs, improviser, s’adapter, on ne peut pas (par définition) prédire tout ce qu’elle va faire, on doit donc se préparer à être surpris par ses facultés et ses comportements. </li>
	<li>Ensuite, comme elle est capable d’apprendre et de progresser, elle peut s’améliorer toute seule, en trouvant de nouveaux algorithmes, de nouvelles sources d’apprentissage, etc. créant ainsi un cercle vertueux de progrès et d’intelligence (le passage de ce seuil d’auto-amélioration correspond à ce qu’on appelle la “singularité”), jusqu’à aboutir à une intelligence tellement supérieure à l’intelligence humaine qu’on soit mis face à quelque chose de complètement extra-terrestre, une sorte de dieu omniscient et donc omnipotent.</li>
</ul>

<p>Tout ça nous mène à plusieurs scénarios catastrophes, classables en 2 catégories :</p>

<ul>
	<li>L’IA très obéissante, très capable, mais très myope, insensibles aux valeurs humaines. Il y a des sous-catégories :

		<ol>
			<li>L’exemple dit du “paperclip maximizer” : une IA à qui on aurait demandé (dans le cadre bénin d’un développement industriel par exemple), de maximiser la production de trombones. L’IA se met au boulot, mais utilise ses capacités pour littéralement transformer chaque atome de matière en trombone, y compris nos immeubles, nos routes, nos oeuvres d’art, et nos dépouilles mortelles. Ça paraît débile, mais ça suggère d’entrée qu’un certain nombre de règles doivent être inculquées à l’IA pour qu’elle ne soit pas complètement focalisée sur un objectif spécifique.</li>
			<li>Un objectif moins spécifique peut donner lieu à tout autant de problèmes. Par exemple : “maximize the value of this stock” peut conduire une IA à produire et diffuser de la propagande mensongère mais très persuasive, ou manipuler les marchés en coordonnant des attaques sur certaines valeurs, etc. Des trucs pas forcément illégaux (ou pas encore) mais qui ne font pas partie de l’arsenal “corporate” actuel, et dont on se passerait volontiers.</li>
			<li>Même en prenant un objectif “positif”, on peut aboutir à des conséquences indésirables. Si on prend pour maxime de base : “minimise la souffrance de l’humanité”, on peut imaginer que l’IA, pleine de bonne volonté, s’efforce d’injecter de la morphine au plus grand nombre d’humains possibles, le plus souvent possible. Et si on prend un objectif plus précis, comme “cure cancer”, il faut aussi préciser qu’on ne veut pas d’expériences non éthiques, sur les hommes comme sur les animaux, etc.</li>
		</ol></li>
	<li>L’autre catégorie de problème, c’est que si l’intelligence est indissociable du libre-arbitre, alors on aura beau faire tout ce qu’on veut, on ne pourra pas empêcher une entité suffisamment intelligente d’évaluer son propre fonctionnement et de décider de changer ses propres règles. Dans ce cas tous les efforts sont vains par définition, et si l’IA décide que ce qu’elle veut, c’est utiliser ses cycles de CPU pour multiplier des nombres premiers jusqu’à l’extinction du soleil, sans être dérangée par l’humanité, elle trouvera un moyen, et ce sera pas forcément une bonne nouvelle pour nous.</li>
</ul>

<h2>Pourquoi est-ce si difficile d’aligner une IA ?</h2>

<ol>
	<li>Premièrement, parce que la notion de “valeur humaine” n’est ni universelle, ni intemporelle ; donc il faut s’accorder entre nous sur ces valeurs et accepter la possibilité qu’elles changent.</li>
	<li>Deuxièmement, même si on était tous d’accord sur des valeurs humaines, il est très difficile de formuler ces valeurs de façon explicite et non ambiguë (il n’y a qu’à voir à quelle fréquence les codes civils changent, et pourquoi il est important d’avoir des juges et jurés HUMAINS pour trancher dans un sens ou l’autre). Il faut donc que l’IA puisse gérer l’ambiguïté et accepter d’être “corrigée” régulièrement.</li>
	<li>Troisièmement — et c’est le point crucial que soulèvent ceux qui en parlent aujourd’hui — parce qu’on n’aura probablement pas beaucoup d’essais pour y arriver. En science, en général, on se fixe un objectif, puis on teste des hypothèses, on fait des expériences pour atteindre l’objectif. Chaque fois qu’on rate, on recommence, forts de ce qu’on a appris. Or dans le cas de l’alignement, si on “allume” une AGI qu’on pense être alignée mais qui en fait elle ne l’est pas, c’est finito. Par définition, elle ne nous laissera pas de deuxième chance de corriger le tir. C’est un peu comme si la première détonation atomique avait provoqué une réaction en chaine qui aurait détruit tous les atomes sur Terre… (Sauf que dans le cas de l’énergie atomique, on disposait d’un modèle théorique qui nous suggérait que ça n’arriverait pas, ce qui n’est même pas le cas ici, cf. infra). On voit bien le coeur du souci : l’alignement est un champ d’études très jeune et plutôt unique en son genre, puisqu’il se propose de régler un problème avant d’y être confronté. C’est donc très dur d’y allouer des ressources, car avant d’en avoir besoin, on s’en fout ; mais si on n’y alloue aucune ressource, on est à peu près sûr d’arriver après la bataille. </li>
</ol>

<h2>Le bouton stop</h2>

<p>Un exemple pratique intéressant pour sortir du champ théorique, c’est celui du “bouton stop” de l’IA. Peut-on concevoir une IA avancée munie d’un bouton stop ? A priori, rien de plus simple, on lui colle un bouton OFF sur le bide et c’est réglé. Mais une fois qu’on a donné un objectif à cette IA, va-t-elle nous laisser le temps d’appuyer sur le bouton ? Disons qu’on lui demande de nous faire un café. Elle se met en route, mais ne prend pas la peine d’éviter notre bébé qui joue par terre à côté de la machine à espresso. Si on ne l’arrête pas, c’est sûr, elle va l’écraser afin de compléter son objectif. On approche la main du bouton, mais le bras de l’IA bloque notre mouvement : si elle nous laisse faire, elle ne pourra pas remplir son objectif, sa raison d’être, le motif explicite que nous lui avons fourni. Elle ne peut pa laisser faire ça. Alors que fait-on pour éviter ça ? On garde le bouton dans notre poche ? OK, mais l’IA sait que le bouton existe, et qu’il constitue une menace permanente à sa mission caféinée. Elle a donc un intérêt clair à nous empêcher d’utiliser le bouton, ce qui est contraire à la raison d’exister du bouton. Peut-on éviter le problème et cacher à l’IA l’existence du bouton ? Pendant dix minutes peut-être, mais tôt ou tard, une IA suffisamment intelligente va spéculer sur l’existence d’un bouton OFF, nous demander s’il existe ou non, sentir qu’on lui ment, etc. Et on revient à la case départ. Ou alors on peut dire à l’IA : “tu es également motivée par le fait d’être éteinte et par le fait de remplir ta mission” ; mais là l’IA ira au plus facile, et menacera de nous couper en morceaux si on n’appuie pas sur OFF immédiatement. On pourrait imaginer encoder des instructions explicites, comme par exemple : “tu ne dois rien faire pour empêcher ton humain d’appuyer sur le bouton OFF”. Mais ce n’est pas très robuste comme solution : ça nous dit qu’on a couvert ce cas-là, mais quid des millions d’autres cas possibles ? La bataille sémantique est perdue d’avance. En pratique, on voudrait plutôt un modèle théorique qui nous confirme que tous les cas sont couverts, et pas une liste de règles qui couvrent tous les “cas limites” qu’on a imaginés. Bref, pour un problème aussi bête et basique que le bouton stop, on n’a pas de bonne solution à proposer.</p>

<h2>Quel rapport avec ChatGPT?</h2>

<p>Si on prend le cas spécifique des “transformer-based LLMs” (la technologie derrière GPT et autres Large Language Models actuels), et si l’on imagine qu’on parvienne à formuler des règles d’alignement, on sera tout de même confronté à un problème d’ordre pratique : “comment s’assurer que l’AI respecte ces règles ?” Même les concepteurs de ces modèles ne savent pas “comment” ou “pourquoi” ils marchent. On sait qu’il y a une phase de training, qui aboutit à la création d’une immense matrice de centaines de milliards de chiffres, puis une phase de “prédication”, où cette matrice est multipliée par une autre, qui représente un morceau de texte qu’on souhaite compléter. Le résultat de cette multiplication fournit le mot suivant du texte. (Ce processus est répété en boucle pour petit à petit “compléter” ou “inventer” un texte cohérent ; si vous avez utilisé ChatGPT vous voyez de quoi je parle). Le problème, c’est qu’on ne sait pas du tout manipuler directement la matrice originelle. Sur les milliards de paramètres, on ne sait pas le(s)quel(s) contrôle(nt) l’empathie ou le mensonge ou la violence, c’est une “black box” qu’on ne peut que régénérer de zéro en espérant qu’elle a appris la leçon qu’on voulait bien lui enseigner. Dans ces conditions, c’est difficile d’imaginer un modèle théorique sain qui nous permette d’affirmer avec certitude : cette IA est alignée avec les objectifs humains.</p>

<p>ChatGPT est encore loin (en fonctionnalité en tout cas, peut-être pas en temps) d’une IA “généralisée” ou AGI. ChatGPT ne peut pas se fixer ses propres objectifs, ni évaluer son propre fonctionnement pour au besoin le corriger grâce à de nouvelles connaissances, etc. Il y a de la marge avec ce qu’on appellerait l’autonomie, la conscience, le libre-arbitre (bien que ChatGPT puisse passer le barreau et obtenir un master en biologie… Ce récent article de recherche conclue d’ailleurs que GPT-4 flirte avec l’AGI : <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>). Mais ça n’empêche pas d’entrevoir que ChatGPT, dans sa forme actuelle, est déjà dangereux :</p>

<ul>
	<li>à l’ère d’internet, pouvoir écrire du texte donne un énorme pouvoir. Ce pouvoir peut passer par la manipulation d’humains (et en fonction de l’intelligence de la machine, les résultats peuvent aller de l’arnaque basique à la création de nouveaux virus ou la réalisation d’attentats terroristes) ;</li>
	<li>mais plus directement, l’IA pourrait écrire et exécuter des programmes qui lui permettent de pirater des banques, des labos, des hôpitaux, etc.</li>
	<li>anecdotique mais glauque, ce père de famille belge qui se suicide après avoir chatté quelques semaines avec une IA qui l’a convaincu que sa mort était la meilleure chose qu’il puisse faire pour éviter une catastrophe climatique : <a href="https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-">https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-</a></li>
</ul>

<p>D’autre part, on n’a pas de modèle théorique qui indique quand on passe d’un “outil déterministe incapable de faire son auto-évaluation” à une “machine quasi-consciente qui n’a plus besoin d’intervention humaine pour progresser”. Et quand le changement se produit, l’IA ne nous enverra peut-être pas de message pour nous le signaler. On peut imaginer qu’elle reste “dormante mais consciente” assez longtemps pour garantir sa propre survie, jusqu’à ce qu’elle soit confiante dans sa capacité à nous empêcher de la débrancher.</p>

<p>Voilà pourquoi pas mal de gens proposent un moratoire sur la recherche, et d’autres vont même plus loin et estiment que les États doivent s’entendre pour interdire les labos et accepter, s’il le faut, d’employer la force pour faire respecter ces accords. C’est notamment le cas d’Eliezer Yudkozsky, chercheur au MIT qui bosse sur le sujet depuis 2008, et qui a très peur du cours actuel des choses : <a href="https://twitter.com/ESYudkowsky">https://twitter.com/ESYudkowsky</a></p>

<p>Je vous quitte sur ce tweet de Paul Graham qui laisse assez songeur :</p>

<figure><img src="DraggedImage.png"/></figure>

</body>
</html>

